{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, string\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "# specify directory to load\n",
    "directory = '/home/chandresh/ckm/data/movies review/review_polarity/txt_sentoken/'\n",
    "vocab_dict ='movie_dict.txt'\n",
    "outfile    = 'processed_review.txt'\n",
    "url =\"http://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"films adapted from comic books have had plenty of success , whether they're about superheroes ( batman , superman , spawn ) , or geared toward kids ( casper ) or the arthouse crowd ( ghost world ) , but there's never really been a comic book like from hell before . \\n\", \"for starters , it was created by alan moore ( and eddie campbell ) , who brought the medium to a whole new level in the mid '80s with a 12-part series called the watchmen . \\n\", 'to say moore and campbell thoroughly researched the subject of jack the ripper would be like saying michael jackson is starting to look a little odd . \\n', 'the book ( or \" graphic novel , \" if you will ) is over 500 pages long and includes nearly 30 more that consist of nothing but footnotes . \\n', \"in other words , don't dismiss this film because of its source . \\n\", \"if you can get past the whole comic book thing , you might find another stumbling block in from hell's directors , albert and allen hughes . \\n\", \"getting the hughes brothers to direct this seems almost as ludicrous as casting carrot top in , well , anything , but riddle me this : who better to direct a film that's set in the ghetto and features really violent street crime than the mad geniuses behind menace ii society ? \\n\", \"the ghetto in question is , of course , whitechapel in 1888 london's east end . \\n\", 'it\\'s a filthy , sooty place where the whores ( called \" unfortunates \" ) are starting to get a little nervous about this mysterious psychopath who has been carving through their profession with surgical precision . \\n', 'when the first stiff turns up , copper peter godley ( robbie coltrane , the world is not enough ) calls in inspector frederick abberline ( johnny depp , blow ) to crack the case . \\n', 'abberline , a widower , has prophetic dreams he unsuccessfully tries to quell with copious amounts of absinthe and opium . \\n', \"upon arriving in whitechapel , he befriends an unfortunate named mary kelly ( heather graham , say it isn't so ) and proceeds to investigate the horribly gruesome crimes that even the police surgeon can't stomach . \\n\", \"i don't think anyone needs to be briefed on jack the ripper , so i won't go into the particulars here , other than to say moore and campbell have a unique and interesting theory about both the identity of the killer and the reasons he chooses to slay . \\n\", \"in the comic , they don't bother cloaking the identity of the ripper , but screenwriters terry hayes ( vertical limit ) and rafael yglesias ( les mis ? rables ) do a good job of keeping him hidden from viewers until the very end . \\n\", \"it's funny to watch the locals blindly point the finger of blame at jews and indians because , after all , an englishman could never be capable of committing such ghastly acts . \\n\", 'and from hell\\'s ending had me whistling the stonecutters song from the simpsons for days ( \" who holds back the electric car/who made steve guttenberg a star ? \" ) . \\n', \"don't worry - it'll all make sense when you see it . \\n\", \"now onto from hell's appearance : it's certainly dark and bleak enough , and it's surprising to see how much more it looks like a tim burton film than planet of the apes did ( at times , it seems like sleepy hollow 2 ) . \\n\", \"the print i saw wasn't completely finished ( both color and music had not been finalized , so no comments about marilyn manson ) , but cinematographer peter deming ( don't say a word ) ably captures the dreariness of victorian-era london and helped make the flashy killing scenes remind me of the crazy flashbacks in twin peaks , even though the violence in the film pales in comparison to that in the black-and-white comic . \\n\", \"oscar winner martin childs' ( shakespeare in love ) production design turns the original prague surroundings into one creepy place . \\n\", 'even the acting in from hell is solid , with the dreamy depp turning in a typically strong performance and deftly handling a british accent . \\n', \"ians holm ( joe gould's secret ) and richardson ( 102 dalmatians ) log in great supporting roles , but the big surprise here is graham . \\n\", \"i cringed the first time she opened her mouth , imagining her attempt at an irish accent , but it actually wasn't half bad . \\n\", 'the film , however , is all good . \\n', '2 : 00 - r for strong violence/gore , sexuality , language and drug content \\n']\n"
     ]
    }
   ],
   "source": [
    "with open(directory+'pos/'+'cv000_29590.txt','r') as fid:\n",
    "    print(fid.readlines())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed  1000  docs\n",
      "processed  2000  docs\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Step 1. Data loading\n",
    "\"\"\"\n",
    "data=[]\n",
    "def process_docs(directory):\n",
    "    for filename in os.listdir(directory):\n",
    "\t\t# skip files that do not have the right extension\n",
    "        if not filename.endswith(\".txt\"):\n",
    "            continue\t\t\n",
    "\t\t# create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "\t\t# load document\n",
    "        with open(path,'r') as fid:\n",
    "           data.append(fid.read())\n",
    "        #print(\"loaded:\", filename)          \n",
    "    print(\"processed \",len(data), \" docs\")\n",
    "\n",
    "process_docs(directory+'pos')\n",
    "process_docs(directory+'neg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "step 2. data cleaning\n",
    "\n",
    "Remove punctuation from words (e.g. ‘what’s’).\n",
    "Removing tokens that are just punctuation (e.g. ‘-‘).\n",
    "Removing tokens that contain numbers (e.g. ’10/10′).\n",
    "Remove tokens that have one character (e.g. ‘a’).\n",
    "Remove tokens that don’t have much meaning (e.g. ‘and’)\n",
    "\n",
    "Some ideas:\n",
    "\n",
    "We can filter out punctuation from tokens using the string translate() function.\n",
    "We can remove tokens that are just punctuation or contain numbers by using an isalpha() check on each token.\n",
    "We can remove English stop words using the list loaded using NLTK.\n",
    "We can filter out short tokens by checking their length.\n",
    "\"\"\"\n",
    "def clean_doc(data):\n",
    "    # split into tokens by white space\n",
    "    for i,doc in enumerate(data):\n",
    "        tokens = doc.split()\n",
    "        # remove punctuation from each token\n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        tokens = [w.translate(table) for w in tokens]\n",
    "        # remove remaining tokens that are not alphabetic\n",
    "        tokens = [word for word in tokens if word.isalpha()]\n",
    "        # filter out stop words\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [w for w in tokens if not w in stop_words]\n",
    "        # filter out short tokens\n",
    "        tokens = [word for word in tokens if len(word) > 1]\n",
    "        #print(tokens)\n",
    "        data[i]=tokens\n",
    "\n",
    "# clean the doc\n",
    "clean_doc(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rated', 'strong', 'language', 'sexual', 'dialogue', 'drug', 'use', 'crude', 'humor', 'violence', 'brief', 'nudity', 'starring', 'ben', 'affleck', 'matt', 'damon', 'linda', 'fiorentino', 'salma', 'hayek', 'alan', 'rickman', 'chris', 'rock', 'kevin', 'smith', 'jason', 'mewes', 'jason', 'lee', 'george', 'carlin', 'alanis', 'morissette', 'running', 'time', 'minutes', 'huge', 'fan', 'kevin', 'smith', 'expecting', 'lot', 'newest', 'project', 'dogma', 'might', 'kevins', 'best', 'work', 'date', 'funny', 'smart', 'foulmouthed', 'dialogue', 'unexpectedly', 'serious', 'undertone', 'besides', 'going', 'god', 'jesus', 'christ', 'actually', 'tries', 'tell', 'people', 'thing', 'rather', 'isnt', 'little', 'movie', 'premiered', 'sundance', 'little', 'film', 'called', 'clerks', 'little', 'film', 'went', 'become', 'huge', 'video', 'sensation', 'really', 'cant', 'meet', 'person', 'green', 'earth', 'seen', 'clerks', 'hailed', 'critics', 'one', 'best', 'movies', 'year', 'soon', 'sequel', 'coming', 'kevin', 'made', 'another', 'movie', 'mallrats', 'flopped', 'horribly', 'thought', 'best', 'movie', 'ever', 'made', 'pretty', 'good', 'little', 'flick', 'panned', 'critics', 'hailed', 'audiences', 'chasing', 'amy', 'came', 'kevins', 'biggest', 'success', 'date', 'get', 'great', 'little', 'movie', 'name', 'dogma', 'wow', 'cast', 'ok', 'plot', 'movie', 'linda', 'fiorentino', 'plays', 'regular', 'woman', 'works', 'abortion', 'clinic', 'one', 'day', 'house', 'metatron', 'voice', 'god', 'appears', 'house', 'much', 'surprise', 'tells', 'must', 'stop', 'two', 'fallen', 'angels', 'matt', 'damon', 'ben', 'affleck', 'getting', 'back', 'heaven', 'ways', 'church', 'ok', 'get', 'told', 'find', 'two', 'prophets', 'one', 'talks', 'lot', 'one', 'doesnt', 'however', 'gets', 'jay', 'silent', 'bob', 'instead', 'give', 'us', 'great', 'performances', 'way', 'new', 'jersey', 'jay', 'silent', 'bob', 'try', 'stop', 'two', 'angels', 'apostle', 'falls', 'sky', 'chris', 'rock', 'helps', 'guide', 'find', 'angels', 'salma', 'hayek', 'plays', 'serendipity', 'also', 'helps', 'along', 'way', 'azrael', 'jason', 'lee', 'demon', 'trying', 'help', 'two', 'angels', 'win', 'bethany', 'linda', 'fiorentino', 'use', 'help', 'people', 'try', 'stop', 'angels', 'cheat', 'way', 'back', 'heaven', 'leading', 'violent', 'bloody', 'end', 'make', 'think', 'twice', 'faith', 'movie', 'everything', 'rolled', 'one', 'offensive', 'offbeat', 'smart', 'sassy', 'comedy', 'surprisingly', 'great', 'message', 'end', 'ends', 'feelgood', 'movie', 'would', 'known', 'kevin', 'soft', 'side', 'could', 'make', 'us', 'know', 'faithful', 'person', 'get', 'offended', 'movie', 'really', 'take', 'heart', 'yes', 'may', 'target', 'catholic', 'community', 'bash', 'lot', 'also', 'seems', 'rather', 'go', 'god', 'heaven', 'actually', 'goes', 'god', 'kevin', 'gives', 'message', 'god', 'review', 'may', 'sound', 'confusing', 'first', 'really', 'way', 'understand', 'see', 'excellent', 'movie', 'kevin', 'goes', 'new', 'lengths', 'hilarious', 'comedy', 'gives', 'us', 'great', 'characters', 'much', 'background', 'study', 'rather', 'start', 'know', 'characters', 'watch', 'movie', 'course', 'best', 'funniest', 'duo', 'history', 'made', 'fall', 'chair', 'laughing', 'friends', 'jay', 'silent', 'bob', 'probably', 'best', 'characters', 'movies', 'like', 'starstudded', 'cast', 'amazing', 'cameos', 'george', 'carlin', 'jeanne', 'garafolo', 'may', 'start', 'lose', 'track', 'script', 'written', 'kevin', 'smith', 'many', 'fwords', 'course', 'strong', 'emphasis', 'sexual', 'talk', 'also', 'seems', 'concentrate', 'important', 'rather', 'isnt', 'makes', 'us', 'laugh', 'time', 'think', 'chris', 'rock', 'gives', 'hilarious', 'performance', 'gets', 'character', 'well', 'classic', 'lines', 'linda', 'fiorentino', 'perfect', 'role', 'makes', 'us', 'believe', 'really', 'course', 'jason', 'mewes', 'kevin', 'smith', 'great', 'job', 'jay', 'silent', 'bob', 'priceless', 'lines', 'jay', 'well', 'silent', 'bob', 'alan', 'rickman', 'jason', 'lee', 'salma', 'hayek', 'round', 'cast', 'matt', 'damon', 'ben', 'affleck', 'starring', 'roles', 'oscar', 'winners', 'classic', 'good', 'hunting', 'give', 'great', 'performances', 'matt', 'damon', 'seems', 'little', 'held', 'back', 'give', 'could', 'ben', 'however', 'got', 'much', 'character', 'well', 'story', 'movies', 'big', 'flaw', 'unfunny', 'lines', 'slow', 'parts', 'go', 'along', 'however', 'direction', 'kevin', 'script', 'kevin', 'well', 'priceless', 'performance', 'many', 'cast', 'members', 'dogma', 'stands', 'new', 'classic', 'think', 'safely', 'compare', 'clerks', 'funny', 'times', 'end', 'becomes', 'serious', 'movie', 'important', 'message', 'may', 'agree', 'cant', 'deny', 'dogma', 'offend', 'people', 'especially', 'deeply', 'religious', 'people', 'open', 'minded', 'want', 'see', 'funny', 'material', 'movie', 'kevin', 'smith', 'fans', 'must', 'see']\n"
     ]
    }
   ],
   "source": [
    "#print cleand doc\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46557\n",
      "[('film', 8860), ('one', 5521), ('movie', 5440), ('like', 3553), ('even', 2555), ('good', 2320), ('time', 2283), ('story', 2118), ('films', 2102), ('would', 2042), ('much', 2024), ('also', 1965), ('characters', 1947), ('get', 1921), ('character', 1906), ('two', 1825), ('first', 1768), ('see', 1730), ('well', 1694), ('way', 1668), ('make', 1590), ('really', 1563), ('little', 1491), ('life', 1472), ('plot', 1451), ('people', 1420), ('movies', 1416), ('could', 1395), ('bad', 1374), ('scene', 1373), ('never', 1364), ('best', 1301), ('new', 1277), ('many', 1268), ('doesnt', 1267), ('man', 1266), ('scenes', 1265), ('dont', 1210), ('know', 1207), ('hes', 1150), ('great', 1141), ('another', 1111), ('love', 1089), ('action', 1078), ('go', 1075), ('us', 1065), ('director', 1056), ('something', 1048), ('end', 1047), ('still', 1038)]\n",
      "46557\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Step 3. Build the vocabulary\n",
    "\"\"\"\n",
    " # define vocab\n",
    "vocab = Counter()\n",
    "\n",
    "for doc in data:\n",
    "    vocab.update(doc)\n",
    "# print lenngth of the vocab\n",
    "print(len(vocab))    \n",
    "# print the top most_common words\n",
    "print(vocab.most_common(50))\n",
    "# print lenngth of the vocab\n",
    "print(len(vocab))  \n",
    "# keep vocab with words whose freq is at least 5\n",
    "min_freq = 5\n",
    "vocab = [w for w, c in vocab.items() if c>=min_freq]  \n",
    "\n",
    "# save the vocab\n",
    "with open(directory+vocab_dict,'w') as fid:\n",
    "    lines =\"\\n\".join(vocab)\n",
    "    fid.write(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "step 4. Save the prepared data for modeling\n",
    "Q. why to do that?\n",
    "A. Decouple the data preparation from modeling\n",
    "\n",
    "Q. How to use dict to clean the reviews\n",
    "A. here are the steps:\n",
    "    1. Load dict\n",
    "    2. process each doc, remvoing tokens not in dict\n",
    "    3. save the doc\n",
    "\"\"\"\n",
    "lines=''\n",
    "for doc in data:\n",
    "    tokens = [w for w in doc if w in vocab]\n",
    "    line = \" \".join(tokens)\n",
    "    lines+=line+'\\n'\n",
    "    \n",
    "with open(directory+outfile,'w') as fid:\n",
    "    fid.write(lines)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "# specify directory to load\n",
    "directory = '/home/chandresh/ckm/data/movies review/review_polarity/txt_sentoken/'\n",
    "inpfile    = 'processed_review.txt'\n",
    "vocab_dict ='movie_dict.txt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Step 1. load the data\n",
    "\"\"\"\n",
    "data =[]\n",
    "with open(directory+inpfile,'r') as fid:\n",
    "    for doc in fid:\n",
    "         tokens = doc.strip('\\n')\n",
    "         data.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rated strong language sexual dialogue drug use crude humor violence brief nudity starring ben affleck matt damon linda fiorentino salma hayek alan rickman chris rock kevin smith jason mewes jason lee george carlin running time minutes huge fan kevin smith expecting lot newest project dogma might kevins best work date funny smart foulmouthed dialogue unexpectedly serious besides going god jesus christ actually tries tell people thing rather isnt little movie premiered sundance little film called clerks little film went become huge video sensation really cant meet person green earth seen clerks hailed critics one best movies year soon sequel coming kevin made another movie mallrats flopped horribly thought best movie ever made pretty good little flick panned critics hailed audiences chasing amy came kevins biggest success date get great little movie name dogma wow cast ok plot movie linda fiorentino plays regular woman works abortion clinic one day house voice god appears house much surprise tells must stop two fallen angels matt damon ben affleck getting back heaven ways church ok get told find two prophets one talks lot one doesnt however gets jay silent bob instead give us great performances way new jersey jay silent bob try stop two angels apostle falls sky chris rock helps guide find angels salma hayek plays serendipity also helps along way jason lee demon trying help two angels win bethany linda fiorentino use help people try stop angels cheat way back heaven leading violent bloody end make think twice faith movie everything rolled one offensive offbeat smart sassy comedy surprisingly great message end ends feelgood movie would known kevin soft side could make us know faithful person get offended movie really take heart yes may target catholic community bash lot also seems rather go god heaven actually goes god kevin gives message god review may sound confusing first really way understand see excellent movie kevin goes new lengths hilarious comedy gives us great characters much background study rather start know characters watch movie course best funniest duo history made fall chair laughing friends jay silent bob probably best characters movies like cast amazing cameos george carlin jeanne may start lose track script written kevin smith many course strong emphasis sexual talk also seems concentrate important rather isnt makes us laugh time think chris rock gives hilarious performance gets character well classic lines linda fiorentino perfect role makes us believe really course jason mewes kevin smith great job jay silent bob priceless lines jay well silent bob alan rickman jason lee salma hayek round cast matt damon ben affleck starring roles oscar winners classic good hunting give great performances matt damon seems little held back give could ben however got much character well story movies big flaw unfunny lines slow parts go along however direction kevin script kevin well priceless performance many cast members dogma stands new classic think safely compare clerks funny times end becomes serious movie important message may agree cant deny dogma offend people especially deeply religious people open minded want see funny material movie kevin smith fans must see\n"
     ]
    }
   ],
   "source": [
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "step 2. load the dictionary\n",
    "\"\"\"\n",
    "vocabulary=defaultdict(int)\n",
    "\n",
    "with open(directory+vocab_dict,'r') as fid:\n",
    "    words = fid.readlines()\n",
    "    words = [w.strip('\\n') for w in words]\n",
    "    vocabulary = {w:i for i, w in enumerate(words)}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "step 3. Using tf-idf model for encoding documents\n",
    "\"\"\"\n",
    "\n",
    "# define data matrix X and label array y\n",
    "pos_label = [1]*1000\n",
    "neg_label = [0]*1000\n",
    "y = np.array(pos_label+neg_label)\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,1), vocabulary=vocabulary)\n",
    "X = tfidf.fit_transform(data)\n",
    "\n",
    "# clear data to free up space\n",
    "del vocabulary\n",
    "del data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data into train/test\n",
    "# step 1. shuffle the data\n",
    "n,d = X.shape\n",
    "index = list(range(0,n))\n",
    "random.shuffle(index)\n",
    "\n",
    "X = X[index, :]\n",
    "y = y[index]\n",
    "\n",
    "# step 2. split into train/test in 2/3-1/3\n",
    "Xtrain = X[:1500,:]\n",
    "ytrain = y[:1500]\n",
    "\n",
    "Xtest = X[1500:,:]\n",
    "ytest = y[1500:]\n",
    "\n",
    "del X\n",
    "del y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1500, 14803)\n"
     ]
    }
   ],
   "source": [
    "print(Xtrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  0.6931066665423955\n",
      "loss:  0.6557437667477811\n",
      "loss:  0.6225077958395836\n",
      "loss:  0.5927778675403912\n",
      "loss:  0.5660398731625527\n",
      "loss:  0.5418622769752304\n",
      "loss:  0.5198852255815232\n",
      "loss:  0.4998092593566594\n",
      "loss:  0.4813849306187443\n",
      "loss:  0.4644038283644683\n",
      "accuracy 0.83\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "custome logistic regression\n",
    "\"\"\"\n",
    "class LogisticRegression:\n",
    "    def __init__(self, lr=0.01, num_iter=10000, fit_intercept=True, verbose=True):\n",
    "        self.lr = lr\n",
    "        self.num_iter = num_iter\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.verbose = verbose\n",
    "    \n",
    "    def __add_intercept(self, X):\n",
    "        intercept = np.ones((X.shape[0], 1))\n",
    "        return np.concatenate((intercept, X), axis=1)\n",
    "    \n",
    "    def __sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    def __loss(self, h, y):\n",
    "        return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        if self.fit_intercept:\n",
    "            X = self.__add_intercept(X)\n",
    "        \n",
    "        # weights initialization\n",
    "        self.w = np.zeros(X.shape[1])\n",
    "        \n",
    "        for i in range(self.num_iter):\n",
    "            z = np.dot(X, self.w)\n",
    "            h = self.__sigmoid(z)\n",
    "            gradient = np.dot(X.T, (h - y)) / y.size\n",
    "            self.w -= self.lr * gradient\n",
    "            \n",
    "            if(self.verbose == True and i % 1000 == 0):\n",
    "                z = np.dot(X, self.w)\n",
    "                h = self.__sigmoid(z)\n",
    "                print('loss: ',self.__loss(h, y))\n",
    "    def predict_prob(self, X):\n",
    "        if self.fit_intercept:\n",
    "            X = self.__add_intercept(X)\n",
    "    \n",
    "        return self.__sigmoid(np.dot(X, self.w))\n",
    "    \n",
    "    def predict(self, X, threshold=0.5):\n",
    "        return self.predict_prob(X) >= threshold\n",
    "\n",
    "#Evaluation\n",
    "model = LogisticRegression(lr=0.1, num_iter=10000)\n",
    "Xtrain =Xtrain.toarray()\n",
    "Xtest = Xtest.toarray()\n",
    "model.fit(Xtrain, ytrain)\n",
    "preds = model.predict(Xtest)\n",
    "# accuracy\n",
    "print('accuracy',(preds == ytest).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 0.85\n",
      "Confusion matrix, without normalization\n",
      "[[213  33]\n",
      " [ 42 212]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "logreg.fit(Xtrain, ytrain)\n",
    "#logreg.score(Xtrain, ytrain)\n",
    "#logreg.score(Xtest, ytest)\n",
    "y_hat = logreg.predict(Xtest)\n",
    "#print accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"test accuracy:\",accuracy_score(ytest,y_hat))\n",
    "\n",
    "##print (confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from util import plot_confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(ytest, y_hat)\n",
    "np.set_printoptions(precision=2)\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=np.unique(y_hat),title='Confusion matrix, without normalization')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
